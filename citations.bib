@article{1,
    author = {Denison, David G. T. and Mallick, Bani K. and Smith, Adrian F. M.},
    title = "{A Bayesian CART algorithm}",
    journal = {Biometrika},
    volume = {85},
    number = {2},
    pages = {363-377},
    year = {1998},
    month = {06},
    issn = {0006-3444},
    doi = {10.1093/biomet/85.2.363},
    url = {https://doi.org/10.1093/biomet/85.2.363},
    eprint = {https://academic.oup.com/biomet/article-pdf/85/2/363/846507/85-2-363.pdf},
}

@article{2,
author = {Loh, Wei-Yin},
year = {2011},
month = {01},
pages = {14 - 23},
title = {Classification and Regression Trees},
volume = {1},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
doi = {10.1002/widm.8}
}

@book{3,
  author = {Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.},
  publisher = {Wadsworth and Brooks},
  title = {Classification and Regression Trees},
  year = 1984
}

@book{4,
  title={Machine Learning with R: Expert techniques for predictive modeling to solve all your data analysis problems},
  author={Lantz, B.},
  isbn={9781784394523},
  url={https://books.google.de/books?id=ZaJNCgAAQBAJ},
  year={2015},
  publisher={Packt Publishing}
}

@Article{5,
author={Mingers, John},
title={An empirical comparison of selection measures for decision-tree induction},
journal={Machine Learning},
year={1989},
month={Mar},
day={01},
volume={3},
number={4},
pages={319-342},
issn={1573-0565},
doi={10.1007/BF00116837},
url={https://doi.org/10.1007/BF00116837}
}

@article{6,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2669832},
 author = {Hugh A. Chipman and Edward I. George and Robert E. McCulloch},
 journal = {Journal of the American Statistical Association},
 number = {443},
 pages = {935--948},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian CART Model Search},
 volume = {93},
 year = {1998}
}

@article{7,
   title={BART: Bayesian additive regression trees},
   volume={4},
   ISSN={1932-6157},
   url={http://dx.doi.org/10.1214/09-AOAS285},
   DOI={10.1214/09-aoas285},
   number={1},
   journal={The Annals of Applied Statistics},
   publisher={Institute of Mathematical Statistics},
   author={Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
   year={2010},
   month={Mar}
}

@article{8,
author = {Bradley Efron and Trevor Hastie and Iain Johnstone and Robert Tibshirani},
title = {{Least angle regression}},
volume = {32},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {407 -- 499},
year = {2004},
doi = {10.1214/009053604000000067},
URL = {https://doi.org/10.1214/009053604000000067}
}

@article{9,
author = {Jerome H. Friedman},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1189 -- 1232},
keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
year = {2001},
doi = {10.1214/aos/1013203451},
URL = {https://doi.org/10.1214/aos/1013203451}
}

@Article{10,
author={Breiman, Leo},
title={Random Forests},
journal={Machine Learning},
year={2001},
month={Oct},
day={01},
volume={45},
number={1},
pages={5-32},
issn={1573-0565},
doi={10.1023/A:1010933404324},
url={https://doi.org/10.1023/A:1010933404324}
}

@inproceedings{11,
  title={A decision-theoric generalization of online learning and an application to boosting},
  author={Freund, Y and Schapire, RE},
  booktitle={Proc. Of the 2th European Conf. on Computational Learning Theory, Rochester, NY, ACM Press},
  pages={202--216},
  year={1990}
}

@article{12,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@phdthesis{13,
  title={Bayesian Computations via MCMC, with Applications to Big Data and Spatial Data},
  author={Entezari, Reihaneh},
  year={2018}
}

@article{14,
  title={bartMachine: Machine learning with Bayesian additive regression trees},
  author={Kapelner, Adam and Bleich, Justin},
  journal={arXiv preprint arXiv:1312.2171},
  year={2013}
}

@article{15,
  title={Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images},
  author={Geman, Stuart and Geman, Donald},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}

@article{16,
  title={A multimethod approach for county-scale geospatial analysis of emerging infectious diseases: a cross-sectional case study of COVID-19 incidence in Germany},
  author={Scarpone, Christopher and Brinkmann, Sebastian T and Gro{\ss}e, Tim and Sonnenwald, Daniel and Fuchs, Martin and Walker, Blake Byron},
  journal={International journal of health geographics},
  volume={19},
  number={1},
  pages={1--17},
  year={2020},
  publisher={BioMed Central}
}

@online{17,
author = {Lopez, Hedibert F., and Paulo Marques},
title = {Bayesian Classification and Regression Trees.},
url = {http://hedibert.org/wp-content/uploads/2018/06/bayesian-CART.pdf},
organization = {Insper Institute of Education and Research.},
year = {2018},
month={June},
}

@online{18,
author = {McCulloch, Robert and Sparapani, Rodney},
title = {Introduction to BART: BART::Wbart.},
url = {http://www.rob-mcculloch.org/BBC/BBC.pdf},
organization = {Medical College of Wisconsin.},
year = {2017},
month={September},
date ={21},
}

@online{19,
author = {Laud, Purushottam, et al.},
title = {Introduction to BART Bootcamp.},
note= {Beer, Brats & Biostats BART Bootcamp, MCW, OktoberFest 2017},
url = {http://www.rob-mcculloch.org/BBC/bootcamptalkPL.pdf},
organization = {Medical College of Wisconsin, Division of Biostatistics.},
year = {2017},
month={October},
}

@Article{20,
author={van Ravenzwaaij, Don
and Cassey, Pete
and Brown, Scott D.},
title={A simple introduction to Markov Chain Monte--Carlo sampling},
journal={Psychonomic Bulletin {\&} Review},
year={2018},
month={Feb},
day={01},
volume={25},
number={1},
pages={143-154},
issn={1531-5320},
doi={10.3758/s13423-016-1015-8},
url={https://doi.org/10.3758/s13423-016-1015-8}
}

@misc{21,
  author= {Mira, Antonietta},
  title= {Introduction to Bayesian Computing.},
  year= {2021},
  organization = {Università della Svizzera italiana.},
  address= {Lugano},
}

@article{22,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@book{23,
  title={The hundred-page machine learning book},
  author={Burkov, Andriy},
  volume={1},
  year={2019},
  publisher={Andriy Burkov Canada}
}

@online{24,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin.},
title = {Datasets Used in SLS: Diabetes Data.},
note= {Statistical Learning with Sparsity},
url = {https://web.stanford.edu/~hastie/StatLearnSparsity/data.html},
organization = {University of Stanford.},
year = {2015},
month={September},
}

@article{25,
  title={Least angle regression},
  author={Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert and others},
  journal={Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}

@article{26,
author = {Scornet, Erwan},
year = {2017},
month = {01},
pages = {144-162},
title = {Tuning parameters in random forests},
volume = {60},
journal = {ESAIM: Proceedings and Surveys},
doi = {10.1051/proc/201760144}
}

@article{27,
  title={Diabetes mellitus: a local and global public health emergency!},
  author={Al-Lawati, Jawad A},
  journal={Oman medical journal},
  volume={32},
  number={3},
  pages={177},
  year={2017},
  publisher={Oman Medical Specialty Board}
}


@article{28,
  title={Understanding the metropolis-hastings algorithm},
  author={Chib, Siddhartha and Greenberg, Edward},
  journal={The american statistician},
  volume={49},
  number={4},
  pages={327--335},
  year={1995},
  publisher={Taylor \& Francis Group}
}

@book{29,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC press}
}

@article{30,
  title={Posterior-based proposals for speeding up Markov chain Monte Carlo},
  author={Pooley, CM and Bishop, Steve C and Doeschl-Wilson, A and Marion, Glenn},
  journal={Royal Society open science},
  volume={6},
  number={11},
  pages={190619},
  year={2019},
  publisher={The Royal Society}
}

@article{31,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/2676659},
 abstract = {We propose general procedures for posterior sampling from additive and generalized additive models. The procedure is a stochastic generalization of the well-known backfitting algorithm for fitting additive models. One chooses a linear operator ("smoother") for each predictor, and the algorithm requires only the application of the operator and its square root. The procedure is general and modular, and we describe its application to nonparametric, semiparametric and mixed models.},
 author = {Trevor Hastie and Robert Tibshirani},
 journal = {Statistical Science},
 number = {3},
 pages = {196--213},
 publisher = {Institute of Mathematical Statistics},
 title = {Bayesian Backfitting},
 volume = {15},
 year = {2000}
}


@InProceedings{32,
  title = 	 {{Particle Gibbs for Bayesian Additive Regression Trees}},
  author = 	 {Lakshminarayanan, Balaji and Roy, Daniel and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {553--561},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/lakshminarayanan15.pdf},
  url = 	 {http://proceedings.mlr.press/v38/lakshminarayanan15.html},
  abstract = 	 {Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is also demand for probabilistic predictions with measures of uncertainty, the Bayesian additive regression trees (BART) model, introduced by Chipman et al. (2010), is increasingly popular. As data sets have grown in size, however, the standard Metropolis–Hastings algorithms used to per- form inference in BART are proving inadequate. In particular, these Markov chains make local changes to the trees and suffer from slow mixing when the data are high- dimensional or the best-fitting trees are more than a few layers deep. We present a novel sampler for BART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a top-down particle filtering algorithm for Bayesian decision trees (Lakshminarayanan et al., 2013). Rather than making local changes to individual trees, the PG sampler proposes a complete tree to fit the residual. Experiments show that the PG sampler outperforms existing samplers in many settings.}
}

@article{33,
  title={Multivariate adaptive regression splines},
  author={Friedman, Jerome H},
  journal={The annals of statistics},
  pages={1--67},
  year={1991},
  publisher={JSTOR}
}

@article{34,
  title={Bayesian treed models},
  author={Chipman, Hugh A and George, Edward I and McCulloch, Robert E},
  journal={Machine Learning},
  volume={48},
  number={1},
  pages={299--320},
  year={2002},
  publisher={Springer}
}